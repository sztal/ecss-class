{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (deadline 10.01.2020 23:59:59)\n",
    "Write solutions for the homework exercises in this notebook. Once the work is done download the notebook file (`File > Download .ipynb`) rename it properly so it follows a template `HW2_<SURNAME>_<NAME>.ipynb` and send the file to us. Please, for exercise 1 also attach a JSON line file with the scrapped data. Use one (or preferably both) of the following e-mails:\n",
    "\n",
    "* <stalaga@uw.edu.pl>\n",
    "* <m.biesaga@uw.edu.pl>\n",
    "\n",
    "Remember that you can contact us if you have any problems. You can describe your problems in the `hw1` channel or in private messages to us on Slack. You can also write normal e-mails. Moreover, you can also visit us in the ISS on the fourth floor (room 415). Usually, at least one of us is there after 11/12 for at least a few hours. Although it is best to set up a meeting earlier via e-mail or a private message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW2 | Exercise 1\n",
    "You all should be able to get the file `links.txt`  our [Google Drive](https://drive.google.com/open?id=1v7jpE82eSJy3XPfvaIAteh6N_MnSu4M7). It contains ten links from the Russia Today website ([www.rt.com](https://www.rt.com)). Your task is to:\n",
    "\n",
    "* Read `links.txt` to Python (Google Colab)\n",
    "* Scrap the articles from the given links\n",
    "* Write out the results into `HW2_<SURNAME>_<NAME>.jl`\n",
    "\n",
    "From every website you should get the following information:\n",
    "\n",
    "* Title of the article\n",
    "* Author of the text (if present, if not the value should be None)\n",
    "* Lead of the article (summary)\n",
    "* Date of publication\n",
    "* Text of the article\n",
    "\n",
    "*Reminder:* A JSON line file contains multiple JSONs. As the name suggests each line contains one. Therefore, it looks more or less like in the example below:\n",
    "\n",
    "```python\n",
    "{title: \"Why are Namedays better than Birthdays?\", author: \"M. Biesaga\", date: \"06.12.2019\", lead: \"Scientists from one of the best Universities in the U.S. proved that the discussion about birthdays and namedays is finally over.\"}\n",
    "{title: \"Why is psychology better than sociology\", author: None, date: \"06.12.2019\", lead: \"A fight between psychologists and sociologists.\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read links.txt to Python and remove new line symbol from every line\n",
    "with open('links.txt', 'r') as file:\n",
    "    links = [link.strip() for link in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "## Open a file in write mode\n",
    "with open('HW2_Biesaga_Mikolaj.jl', 'w') as file:\n",
    "\n",
    "    ## Iterate over links\n",
    "    for link in links:\n",
    "        ## Send a get request\n",
    "        response = requests.get(link)\n",
    "        ## Convert to BeatifulSoup object\n",
    "        html = BeautifulSoup(response.content, 'html.parser')\n",
    "        ## Create a mapping to store article data\n",
    "        article = {}\n",
    "        ## Extract title\n",
    "        article['title'] = html.select_one('body div.article h1.article__heading').text.strip()\n",
    "        ## Extract lead\n",
    "        article['lead'] = html.select_one('body div.article div.article__summary').text.strip()\n",
    "        ## Extract date\n",
    "        article['date'] = html.select_one('body div.article span.date_article-header').text.strip()\n",
    "        ## Extract author if the author is known\n",
    "        if html.select_one('body div.article div.blog-autor__name') != None:\n",
    "            article['author'] = html.select_one('body div.article div.blog-autor__name').text.strip()\n",
    "        else:\n",
    "            article['author'] = None\n",
    "        ## Create list for paragraphs\n",
    "        content = [] \n",
    "        ## Iterate over paragraphs\n",
    "        for p in html.select_one('body div.article div.article__text'):\n",
    "            ## Store paragraphs content in the list\n",
    "            content.append(p.text.strip())\n",
    "        ## Store paragraphs content in the mapping (first join by space, and later remove unnecesary space)\n",
    "        article['content'] = \" \".join(content).strip()\n",
    "\n",
    "        ## Stream out every mapping as json in separate lines\n",
    "        file.write(json.dumps(article)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list to store read data\n",
    "articles_read = []\n",
    "\n",
    "## Open the file in read mode\n",
    "with open('HW2_Biesaga_Mikolaj.jl', 'r') as file:\n",
    "    ## Read every line separately and store it in the list\n",
    "    lines = file.readlines()\n",
    "    ## Iterate over strings\n",
    "    for line in lines:\n",
    "        ## Convert a string to mapping and append to the list\n",
    "        articles_read.append(json.loads(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
