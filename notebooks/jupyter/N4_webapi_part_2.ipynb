{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web APIs and HTTP requests: part II\n",
    "\n",
    "Brief reminder. Web API (*Application Programming Interface*) is a web-based (often public or semi-public), a programmable interface that allows third parties to communicate with some digital system in an organized and automated manner.\n",
    "\n",
    "More importantly, the majority of web APIs use the HTTP protocol to communicate with the external world. The HTTP protocol is the standard communication protocol used everywhere around the World Wide Web (this is exactly the protocol that your browser uses to communicate with servers that host websites you use). By this virtue REST API are very universal and flexible as HTTP-based communication is supported by every serious programming language and/or operating system. Moreover, it makes it even possible to use REST APIs via a browser, which is very handy for testing.\n",
    "\n",
    "So, in summary, a web API is:\n",
    "\n",
    "* **Remote.** Users can access the resources from anywhere, provided they have an internet connection.\n",
    "* **Reliable.** The interface exposed to users is stable, which means that it does not change often in time and is largely independent of changes within the system on top of which it sits.\n",
    "* **Programmable.** API can be interacted with based on a predefined set of commands/methods/endpoints (an interface) in a way that can be expressed with a programming language.\n",
    "\n",
    "Moreover, perhaps one of the most important features of a web API is the fact that it is identified by a unique URL and IP number (exactly in the same way as any ordinary website). Every particular method/endpoint in the API can be interacted with by extending the base URL of the API with appropriate query parameters and/or subdirectories. For example:\n",
    "\n",
    "* https://en.wikipedia.org/w/api.php is the base URL of the Wikipedia API.\n",
    "* https://en.wikipedia.org/w/api.php?action=query&list=random&rnnamespace=0&rnlimit=2 is the URL that uses the `query` endpoint and the `list` method nested within it.\n",
    "\n",
    "Let us remind ourselves of the anatomy of URL-based communication with web APIs.\n",
    "\n",
    "The base URL `https://en.wikipedia.org/w/api.php` is a simple, standard URL. Nothing special about except for the fact that it points to the location at which the Wikipedia API lives. Now, note that the `query` action is an extension of the base URL of the following form:\n",
    "\n",
    "* `<BASE_URL> ? <QUERY STRING>`\n",
    "* Where query string is a sequence of key-value pairs of the following form `<key1>=<value1>&<key2>=<value2> ...`.\n",
    "\n",
    "The `?` sign separates the base URL from the query string part. And in our example, the query params (key-value pairs) specify that we want the API to use the `query` endpoint and use it to execute the `list=random` method with the following arguments: `rnamespace=0&rnlimit=2`. The `query` endpoint and all its methods such as `list=random` are properly documented at: `https://en.wikipedia.org/w/api.php?action=help&modules=query` (note that this website is served also through an API query)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Wikipedia API: part II\n",
    "\n",
    "The last time we extracted the list of Wikiprojects and counted them. This time we will try to do something a little bit more involved.\n",
    "\n",
    "1. We gonna take a random sample of 10 Wikipedia articles. There is an endpoint in the Wikipedia for doing just that. However, note because of the sampling each of you will get different results.\n",
    "2. The first step will give us only id numbers and the title of the pages. We will use them to extract the full text of the pages via a different endpoint of the Wikipedia API.\n",
    "3. We will compute word length distributions of the pages. Exactly, we will reuse the code that you developed earlier for the final exercise from notebook 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.\n",
    "\n",
    "First, we have to sample 10 random Wikipedia articles. This should not be too hard since we have a special method for this, so it should be just one simple API call.\n",
    "\n",
    "The method we are looking for is `list=random` and it is defined within the `query` endpoint (`action=query`). We can read more about it [here](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Brandom).\n",
    "\n",
    "**HINT.** Remember that you can view the results of your queries directly in the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick read of the doc page and we can decide that we need only two query parameters:\n",
    "\n",
    "1. `rnamespace=0` (which limits the results to the namespace `0` which is the part of Wikipedia where actual encyclopedic articles live).\n",
    "2. `rnlimit=10` (because we want to extract only 10 random articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above considerations lead us to the following payload\n",
    "# we will want to attach to out query URL.\n",
    "\n",
    "payload = {\n",
    "    'action': 'query',  # since we want to use the `query` endpoint\n",
    "    'list': 'random',   # because we want to use the `random` method\n",
    "    # But we also need to add arguments for the `random` method\n",
    "    'rnnamespace': 0,\n",
    "    'rnlimit': 10,\n",
    "    'format': 'json'    # we need to add it so the data can be read by Python\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we are ready to make the GET request\n",
    "# But first we need to import the requests package\n",
    "import requests as rq\n",
    "# And define our base URL\n",
    "BASE_URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "response = rq.get(BASE_URL, params=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batchcomplete': '',\n",
       " 'continue': {'rncontinue': '0.813433170942|0.813435328434|27642096|0',\n",
       "  'continue': '-||'},\n",
       " 'query': {'random': [{'id': 26816666, 'ns': 0, 'title': 'Phil Chen'},\n",
       "   {'id': 13933129, 'ns': 0, 'title': 'Linda Royster Beito'},\n",
       "   {'id': 1020852,\n",
       "    'ns': 0,\n",
       "    'title': 'Timeline of aviation before the 18th century'},\n",
       "   {'id': 3313918, 'ns': 0, 'title': 'Moral Court'},\n",
       "   {'id': 29749486, 'ns': 0, 'title': 'Grab (peak)'},\n",
       "   {'id': 32601692, 'ns': 0, 'title': 'List of endemic flora of Indonesia'},\n",
       "   {'id': 8589017, 'ns': 0, 'title': 'John Cuthbert'},\n",
       "   {'id': 36377511, 'ns': 0, 'title': 'Kammweg'},\n",
       "   {'id': 28344508, 'ns': 0, 'title': 'Holly Recreation Area'},\n",
       "   {'id': 34890576, 'ns': 0, 'title': 'Cloughgrenan'}]}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see that our response is OK (HTTP response code 200 means 'OK')\n",
    "# So we can extract the data from the response object with `.json()`\n",
    "# method defined on it.\n",
    "data = response.json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so now we have titles of articles and their unique ids. As you can probably imagine we are only interested in the unique ids at this point. Because we will need them to in different endpoint to extract texts of the articles. But how exactly we are going to access them? It is a dictionary so it should not be a major problem to access a single value, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19969580"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['query']['random'][0]['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what exactly happened there? First, we got a mapping with three keys: `batchcomplete`, `continue`, and `query`. We were only interested in the `query` field. Therefore, we typed as follows:\n",
    "```python\n",
    "data['query']\n",
    "```\n",
    "However, it was again a mapping inside a mapping with only one field: `random`. Therefore:\n",
    "```python\n",
    "data['query']['random']\n",
    "```\n",
    "Inside this mapping, we had a five-element list. So to access the first element we typed:\n",
    "```python\n",
    "data['query']['random'][0]\n",
    "```\n",
    "Every element of that list was also a mapping again with three keys: `id`, `ns`, and `title`. We were only interested in the `id` field. So, we just typed:\n",
    "```python\n",
    "data['query']['random'][0]['id']\n",
    "```\n",
    "However, again we could access all the `ids` manually but it would be easier just to use a for-loop. As you probably can imagine we are going to loop over that list because the rest of the fields are going to be the same.\n",
    "\n",
    "```python\n",
    "for page in data['query']['random']:\n",
    "    print(page['id'])\n",
    "```\n",
    "So a loop like this would work fine if we only wanted to print the `ids`. We could even modify it a bit to store the `ids` in the list (it is what we want to do), for example:\n",
    "```python\n",
    "list_ids = []\n",
    "for page in data['query']['random']:\n",
    "    list_ids.append(page['id'])\n",
    "```\n",
    "So first, we would create a list outside of the loop and then use a method `append` to add each value of `page['id']` as the last element of the list. It is doable. But Python offers a smarter way of saving results of the loop in a list. It is called **list comprehension** and in this particular example looks like this:\n",
    "```python\n",
    "page_ids = [ page['id'] for page in data['query']['random'] ]\n",
    "```\n",
    "It does exactly the same as the previous example but in a neater way. The difference is that first you write what is happening in the loop `page['id']` and afterward you define the loop `for page in data['query']['random']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19969580,\n",
       " 39982842,\n",
       " 25699035,\n",
       " 52642931,\n",
       " 53055349,\n",
       " 24133565,\n",
       " 1164662,\n",
       " 40656459,\n",
       " 12533026,\n",
       " 47110862]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From the obtained relatively simply dictionary\n",
    "# We can extract the list of page ids as follows:\n",
    "page_ids = [ page['id'] for page in data['query']['random'] ]\n",
    "page_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.\n",
    "Now we have a nice list of page ids, so we can use it to extract the content of the pages using a different method defined on the `query` endpoint.\n",
    "\n",
    "We will use a so-called _cirrus doc_ endpoint. _Cirrus_ is a system for organizing and storing text documents used by Wikipedia. It does not really matter to us. What matters is the fact that an endpoint like this exists and that it has a particular format.\n",
    "\n",
    "As we said _cirrus doc_ is a method on the `query` endpoint and we can call it with `prop=cirrusdoc`. However, to obtain any data we have also to pass a list of page ids in a proper format.\n",
    "\n",
    "Remember every piece of data that we provide through URL parameters (query string) is always treated as a string. Thanks to this every API can use some convention for defining lists of values. The Wikipedia API uses `|` as the separator, so it uses the following convention:\n",
    "\n",
    "* `<item 1>|<item 2>| ... |<item n>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'19969580|39982842|25699035|52642931|53055349|24133565|1164662|40656459|12533026|47110862'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Thus we have to join our page ids to form a single string\n",
    "page_ids_string = \"|\".join(str(p) for p in page_ids) ## this for loop is written similarly as the previous one\n",
    "page_ids_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'query',\n",
       " 'prop': 'cirrusdoc',\n",
       " 'pageids': '19969580|39982842|25699035|52642931|53055349|24133565|1164662|40656459|12533026|47110862',\n",
       " 'format': 'json'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, the above considerations already enforce a particular form of a payload\n",
    "# that we will have to attach to the request URL.\n",
    "\n",
    "payload = {\n",
    "    'action': 'query',\n",
    "    'prop': 'cirrusdoc',\n",
    "    'pageids': page_ids_string,\n",
    "    'format': 'json'\n",
    "}\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now we are ready to make a request\n",
    "response = rq.get(BASE_URL, params=payload)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batchcomplete', 'query'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And parse the response to a json dictionary\n",
    "data = response.json()\n",
    "# We can look and the top-level keys of the dict\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pages'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We should be interested in the query field, since judging by the name\n",
    "# it should contain the results of our query\n",
    "data['query'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1164662', '12533026', '19969580', '24133565', '25699035', '39982842', '40656459', '47110862', '52642931', '53055349'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Great, now we have only one key on the lower level, so it has to store the data\n",
    "pages = data['query']['pages']\n",
    "pages.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pageid', 'ns', 'title', 'cirrusdoc'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see that the pages dictionary store all the pages we requested identified with their ids\n",
    "# Let us look at the inner keys of sub-dict with data of a single page\n",
    "key = list(pages)[0]\n",
    "pages[key].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It seems that the main data is stored under the `cirrusdoc` key.\n",
    "type(pages[key]['cirrusdoc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'type', 'id', 'version', 'source'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hmm, the cirrusdoc property is a list.\n",
    "# So we have to extract data from it.\n",
    "pages[key]['cirrusdoc'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['template', 'content_model', 'opening_text', 'wiki', 'auxiliary_text', 'language', 'title', 'score', 'text', 'timestamp', 'redirect', 'wikibase_item', 'heading', 'source_text', 'version_type', 'coordinates', 'version', 'external_link', 'namespace_text', 'namespace', 'text_bytes', 'incoming_links', 'category', 'outgoing_link', 'popularity_score', 'create_timestamp'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, finally we see the source key, that must store the actual article content\n",
    "pages[key]['cirrusdoc'][0]['source'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alpha Persei (Latinized from α\\xa0Persei, abbreviated Alpha\\xa0Per, α\\xa0Per), formally named Mirfak (pronounced /ˈmɜːrfæk/ or /ˈmɪərfæk/), is the brightest star in the northern constellation of Perseus, outshining the constellation\\'s best known star, Algol. Alpha Persei has an apparent visual magnitude of 1.8, and is a circumpolar star when viewed from mid-northern latitudes. Alpha Persei lies in the midst of a cluster of stars named as the eponymous Alpha Persei Cluster, or Melotte 20, which is easily visible in binoculars and includes many of the fainter stars in the constellation. Determined distance using the trigonometric parallax, places the star 510 light-years (160 parsecs) from the Sun. The spectrum of Alpha Persei matches a stellar classification of F5\\xa0Ib, revealing it to be a supergiant star in the latter stages of its evolution. It has a similar spectrum to Procyon A, though the latter star is much less luminous. This difference is highlighted in their spectral designation under the Yerkes spectral classification, published in 1943, where stars are ranked on luminosity as well as spectral typing. Procyon A is thus F5\\xa0IV, a subgiant star. Since 1943, the spectrum of Alpha Persei has served as one of the stable anchor points by which other stars are classified. Alpha Persei has about 8.5 times the Sun\\'s mass and has expanded to roughly 60 times the size of the Sun. It is radiating 5,000 times the luminosity of the Sun from its outer atmosphere at an effective temperature of 6,350\\xa0K, which creates the yellow-white glow of an F-type star. In the Hertzsprung–Russell diagram, Alpha Persei lies inside the region in which Cepheid variables are found. It is thus useful in the study of these stars, which are important standard candles. In 2010 evidence was presented of a planet orbiting Mirfak. Radial velocity data from repeated observations of the star found a periodic variation with an amplitude of 70.8 ± 1.6 m/s. The proposed planet is estimated to have a minimum mass of approximately 6.6 times that of Jupiter and an orbital period of 128 days, but the claimed period may not be stable over 20 years so the exoplanet is considered doubtful. Rotational modulation due to surface activity such as starspots seem a more likely explanation of the radial velocity variations. In previous publications, periodic radial velocity variations of 87.7 or 77.7days have been reported but, these have not been confirmed. α Persei is the star\\'s Bayer designation. The star also bore the traditional names Mirfak and Algenib, which are Arabic in origin. The former, meaning \\'Elbow\\' and also written Mirphak, Marfak or Mirzac, comes from the Arabic Mirfaq al-Thurayya, while Algenib, also spelt Algeneb, Elgenab, Gęnib, Chenib or Alchemb, is derived from الجنب al-janb, or الجانب al-jānib, \\'the flank\\' or \\'side\\'. and was also the traditional name for Gamma Pegasi. In 2016, the International Astronomical Union organized a Working Group on Star Names (WGSN) to catalog and standardize proper names for stars. The WGSN\\'s first bulletin of July 2016 included a table of the first two batches of names approved by the WGSN; which included Mirfak for this star (Gamma Pegasi was given the name Algenib). Hinali\\'i is the name of the star in Native Hawaiian astronomy. The name of the star is meant to commemorate a great tsunami and mark the beginning of the migration of Maui. According to some Hawaiian folklore, Hinali\\'i is the point of separation between the Earth and the sky that happened during the creation of the Milky Way. Assemani alluded to a title on the Borgian globe, Mughammid (مغمد), or Muliammir al Thurayya (ملىمرٱلطرى), the Concealer of the Pleiades, which, from its location, may be for this star. This star, together with γ Persei, δ Persei, η Persei, σ Persei and ψ Persei, has been called the Segment of Perseus. In Chinese, 天船 (Tiān Chuán), meaning Celestial Boat, refers to an asterism consisting of α Persei, γ Persei, δ Persei, η Persei, μ Persei, ψ Persei, 48 Persei and HD 27084. Consequently, the Chinese name for α Persei itself is 天船三 (Tiān Chuán sān, English: the Third Star of Celestial Boat.) van Leeuwen, F. (November 2007), \"Validation of the new Hipparcos reduction\", Astronomy and Astrophysics, 474 (2): 653–664, arXiv:0708.1752, Bibcode:2007A&A...474..653V, doi:10.1051/0004-6361:20078357 Lyubimkov, Leonid S.; et al. (February 2010), \"Accurate fundamental parameters for A-, F- and G-type Supergiants in the solar neighbourhood\", Monthly Notices of the Royal Astronomical Society, 402 (2): 1369–1379, arXiv:0911.1335, Bibcode:2010MNRAS.402.1369L, doi:10.1111/j.1365-2966.2009.15979.x Arellano Ferro, A. (October 2010), \"Functional relationships for T_eff and log g in F-G supergiants from uvby-beta photometry\", Revista Mexicana de Astronomía y Astrofísica, 46: 331–338, arXiv:1007.0771, Bibcode:2010RMxAA..46..331A Johnson, H. L.; et al. (1966), \"UBVRIJKL photometry of the bright stars\", Communications of the Lunar and Planetary Laboratory, 4 (99): 99, Bibcode:1966CoLPL...4...99J Mermilliod, J. C.; Mayor, M.; Udry, S. (July 2008), \"Red giants in open clusters. XIV. Mean radial velocities for 1309 stars and 166 open clusters\", Astronomy and Astrophysics, 485 (1): 303–314, Bibcode:2008A&A...485..303M, doi:10.1051/0004-6361:200809664 Nordgren, Tyler E.; et al. (December 1999), \"Stellar Angular Diameters of Late-Type Giants and Supergiants Measured with the Navy Prototype Optical Interferometer\", The Astronomical Journal, 118 (6): 3032–3038, Bibcode:1999AJ....118.3032N, doi:10.1086/301114 Gray, R. O.; Graham, P. W.; Hoyt, S. R. (April 2001), \"The Physical Basis of Luminosity Classification in the Late A-, F-, and Early G-Type Stars. II. Basic Parameters of Program Stars and the Role of Microturbulence\", The Astronomical Journal, 121 (4): 2159–2172, Bibcode:2001AJ....121.2159G, doi:10.1086/319957 Bernacca, P. L.; Perinotto, M. (1970), \"A catalogue of stellar rotational velocities\", Contributi Osservatorio Astronomico di Padova in Asiago, 239 (1): 1, Bibcode:1970CoAsi.239....1B \"IAU Catalog of Star Names\". Retrieved 28 July 2016. \"Mirfak\". Merriam-Webster Dictionary. Kaler, James B., \"Mirfak\", Stars, University of Illinois, retrieved 2012-03-14 Ramanamurthy, G. (2007), Biographical Dictionary of Great Astronomers, Sura Books, p.\\xa0167, ISBN\\xa0978-81-7478-697-5 Garrison, R. F. (December 1993), \"Anchor Points for the MK System of Spectral Classification\", Bulletin of the American Astronomical Society, 25: 1319, Bibcode:1993AAS...183.1710G, retrieved 2012-02-04 Mérand, Antoine; et al. (August 2007), \"Extended Envelopes around Galactic Cepheids. III. Y Ophiuchi and α Persei from Near-Infrared Interferometry with CHARA/FLUOR\", The Astrophysical Journal, 664 (2): 1093–1101, arXiv:0704.1825, Bibcode:2007ApJ...664.1093M, doi:10.1086/518597 Lee, B. -C; Han, I.; Park, M. -G.; Kim, K. -M.; Mkrtichian, D. E. (2012). \"Detection of the 128-day radial velocity variations in the supergiant α Persei. Rotational modulations, pulsations, or a planet?\". Astronomy and Astrophysics. 543: A37. arXiv:1205.3840. Bibcode:2012A&A...543A..37L. doi:10.1051/0004-6361/201118539. Allen, R. H. (1963). Star Names: Their Lore and Meaning (Reprint ed.). New York: Dover Publications. p.\\xa0331. ISBN\\xa0978-0-486-21079-7. Retrieved 2012-09-04. Davis Jr., G. A. (October 1944), \"The Pronunciations, Derivations, and Meanings of a Selected List of Star Names\", Popular Astronomy, 52 (3): 14, Bibcode:1944PA.....52....8D Pegasus \"IAU Working Group on Star Names (WGSN)\". Retrieved 22 May 2016. \"Bulletin of the IAU Working Group on Star Names, No. 1\" (PDF). Retrieved 28 July 2016. \"Astronomer charts skies in Hawaiian\" (PDF), Mālamalama, the Magazine of the University of Hawai\\'i System, 29 (2): 8, May 2004, retrieved 2012-03-14 (in Chinese) AEEA (Activities of Exhibition and Education in Astronomy) 天文教育資訊網 2006 年 7 月 11 日'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bingo!! We see the `text` field. It contains the article text.\n",
    "# This is exactly what we want to extract.\n",
    "pages[key]['cirrusdoc'][0]['source']['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We examined the anatomy of the response of the _cirrus doc_ method in the Wikipedia API. So now we understand it and we can use this new knowledge to automatically extract the content of all the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = [ p['cirrusdoc'][0]['source']['text'] for p in pages.values() ]\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NOTE THAT THE PREVIOUS EXPRESSION\n",
    "## DOES THE SAME AS THE FOLLOWING MORE VERBOSE EXPRESSION\n",
    "articles = []\n",
    "for page_id in pages.keys():\n",
    "  page = pages[page_id]\n",
    "  cirrus = page['cirrusdoc']\n",
    "  page_data = cirrus[0]\n",
    "  source = page_data['source']\n",
    "  text = source['text']\n",
    "  articles.append(text)\n",
    "\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!!! We finally extracted the data we want. Now we can apply our method for computing word length distributions to this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework (deadline: 11.12.2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write solutions for the homework exercises in this notebook. Once the work is done download the notebook file (`File > Download .ipynb`) rename it properly so it follows a template `HW1_<NAME>_<SURNAME>.ipynb` and send the file to us. Use one (or prefereably both) of the following e-mails:\n",
    "\n",
    "* <stalaga@protonmail.com>\n",
    "* <m.biesaga1987@gmail.com>\n",
    "\n",
    "Remember that you can contact us if you have any problems. You can describe your problems in the `hw1` channel or in private messages to us on Slack. You can also write normal e-mails. Moreover, you can also visit us in the ISS on the fourth floor (room 415). Usually at least one of us is there after 11/12 for at least few hours. Although it is best to setup a meeting earlier via e-mail or a private message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW1 | Exercise 1.\n",
    "\n",
    "Read about the `pageviews` method (`prop=pageviews`) in the `query endpoint` ([docpage](https://en.wikipedia.org/w/api.php?action=help&modules=query%2Bpageviews)). Use this method to extract page views data for the pages from the previous exercise (if you want you can sample 10 new pages with the `list=random` method) for the last 60 days.\n",
    "\n",
    "The results will be broken down by single days, so you have to aggregate the results (sum) so they give the total page views count for the entire period of 60 days.\n",
    "\n",
    "Remember that to select pages by page ids you pass `pageids=<id 1>|<id 2>|...|<id n>`. We did a very similar thing when we extracted article content through the `cirrusdoc` method in the Wikipedia API in the previous part of this notebook.\n",
    "\n",
    "Your final output should be a `dict` object that maps page ids to pageviews (total number of pageviews over 60 days). It should look something like this:\n",
    "\n",
    "```python\n",
    "results = {\n",
    "    # page_id: pageviews\n",
    "    153253: 10204,\n",
    "    423423: 101,\n",
    "    11012:  12,\n",
    "    42435:  546,\n",
    "    # and so on\n",
    "}\n",
    "```\n",
    "\n",
    "If you want you can sample 10 pages yourself. Otherwise you may use the following list of page ids that we prepared for you.\n",
    "Sampling pages yourself will give you extra credit (but it is possible to get maximum points without it as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ids = [\n",
    "    19969580,\n",
    "    39982842,\n",
    "    25699035,\n",
    "    52642931,\n",
    "    53055349,\n",
    "    24133565,\n",
    "    1164662,\n",
    "    40656459,\n",
    "    12533026,\n",
    "    47110862\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR SOLUTION IN THIS CHUNK\n",
    "## YOU MAY ADD MORE CHUNKS BELOW,\n",
    "## BUT KEEP THE CHUNKS FOR THE EXERCISE 2\n",
    "## CLEARLY SEPARATED\n",
    "##\n",
    "## TRY TO ADD COMMENTS TO YOUR CODE\n",
    "## THAT DESCRIBE WHAT YOU DO AT A GIVEN STEP OF THE PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 1 | Exercise 2.\n",
    "\n",
    "(this is a pure Python exercise for practice; not related to web APIs)\n",
    "\n",
    "Write a function that takes one argument `n` and prints a simple pyramid of the following form:\n",
    "\n",
    "$n = 3$\n",
    "```\n",
    "  *\n",
    " ***\n",
    "*****\n",
    "```\n",
    "\n",
    "$n = 5$\n",
    "```\n",
    "    *\n",
    "   ***\n",
    "  *****\n",
    " *******\n",
    "*********\n",
    "```\n",
    "\n",
    "Remember that we define a function in Python like this.\n",
    "\n",
    "```python\n",
    "def add_two_numbers(x, y):\n",
    "    return x + y\n",
    "```\n",
    "\n",
    "And that you can print from functions like this.\n",
    "\n",
    "```python\n",
    "def print_a_string_from_function(string):\n",
    "    print(string)\n",
    "```\n",
    "\n",
    "Note that to print something you do not use the `return` statement.\n",
    "\n",
    "HINT. You may want to use the fact that in Python strings can be easily multiplied.\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "'x' * 5 == 'xxxxx'\n",
    "```\n",
    "\n",
    "Note that you can do the same with an ,,empty'' space.\n",
    "\n",
    "```python\n",
    "\" \" * 5 == \"     \"\n",
    "```\n",
    "\n",
    "HINT 2. It may be convenient to use a for loop for printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pyramid(n):\n",
    "    pass # Remove this and fill the function with proper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage:\n",
    "\n",
    "```python\n",
    "print_pyramid(4)\n",
    "```\n",
    "\n",
    "Should print:\n",
    "```\n",
    "   *\n",
    "  ***\n",
    " *****\n",
    "*******\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
